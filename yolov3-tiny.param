7767517
50 50
[layer type] [layer name] [input count] [output count] [input blobs] [output blobs] [layer specific params]
Input			data			0 1 data 0=416 1=416 2=3

Convolution		conv0			1 1 data conv0 0=16 1=3 2=1 3=1 4=1 5=1 6=432
BatchNorm		conv0/bn		1 1 conv0 conv0/bn_out 0=16
Scale			conv0/scale		1 1 conv0/bn_out conv0/scale_out 0=16 1=1
Relu			conv0/relu		1 1 conv0/scale_out conv0/relu_out 0=0.1
Pooling			pool0			1 1 conv0/relu_out pool0_out 0=0 1=2 2=2 3=0 4=0

Convolution		conv1			1 1 pool0_out conv1 0=32 1=3 2=1 3=1 4=1 5=1 6=4608
BatchNorm		conv1/bn		1 1 conv1 conv1/bn_out 0=32
Scale			conv1/scale		1 1 conv1/bn_out conv1/scale_out 0=32 1=1
Relu			conv1/relu		1 1 conv1/scale_out conv1/relu_out 0=0.1
Pooling			pool1			1 1 conv1/relu_out pool1_out 0=0 1=2 2=2 3=0 4=0

Convolution		conv2			1 1 pool1_out conv2 0=64 1=3 2=1 3=1 4=1 5=1 6=18432
BatchNorm		conv2/bn		1 1 conv2 conv2/bn_out 0=64
Scale			conv2/scale		1 1 conv2/bn_out conv2/scale_out 0=64 1=1
Relu			conv2/relu		1 1 conv2/scale_out conv2/relu_out 0=0.1
Pooling			pool2			1 1 conv2/relu_out pool2_out 0=0 1=2 2=2 3=0 4=0

Convolution		conv3			1 1 pool2_out conv3 0=128 1=3 2=1 3=1 4=1 5=1 6=73728
BatchNorm		conv3/bn		1 1 conv3 conv3/bn_out 0=128
Scale			conv3/scale		1 1 conv3/bn_out conv3/scale_out 0=128 1=1
Relu			conv3/relu		1 1 conv3/scale_out conv3/relu_out 0=0.1
Pooling			pool3			1 1 conv3/relu_out pool3_out 0=0 1=2 2=2 3=0 4=0

Convolution		conv4			1 1 pool3_out conv4 0=256 1=3 2=1 3=1 4=1 5=1 6=294912
BatchNorm		conv4/bn		1 1 conv4 conv4/bn_out 0=256
Scale			conv4/scale		1 1 conv4/bn_out conv4/scale_out 0=256 1=1
Relu			conv4/relu		1 1 conv4/scale_out conv4/relu_out 0=0.1
Pooling			pool4			1 1 conv4/relu_out pool4_out 0=0 1=2 2=2 3=0 4=0

Convolution		conv5			1 1 pool4_out conv5 0=512 1=3 2=1 3=1 4=1 5=1 6=1179648
BatchNorm		conv5/bn		1 1 conv5 conv5/bn_out 0=512
Scale			conv5/scale		1 1 conv5/bn_out conv5/scale_out 0=512 1=1
Relu			conv5/relu		1 1 conv5/scale_out conv5/relu_out 0=0.1
Pooling			pool5			1 1 conv5/relu_out pool5_out 0=0 1=2 2=1 3=0 4=0 14=1

Convolution		conv6			1 1 pool5_out conv6 0=1024 1=3 2=1 3=1 4=1 5=1 6=4718592
BatchNorm		conv6/bn		1 1 conv6 conv6/bn_out 0=1024
Scale			conv6/scale		1 1 conv6/bn_out conv6/scale_out 0=1024 1=1
Relu			conv6/relu		1 1 conv6/scale_out conv6/relu_out 0=0.1

Convolution		conv7			1 1 conv6/relu_out conv7 0=256 1=1 2=1 3=1 4=0 5=1 6=262144
BatchNorm		conv7/bn		1 1 conv7 conv7/bn_out 0=256
Scale			conv7/scale		1 1 conv7/bn_out conv7/scale_out 0=256 1=1
Relu			conv7/relu		1 1 conv7/scale_out conv7/relu_out 0=0.1

Convolution		conv8			1 1 conv7/relu_out conv8 0=512 1=3 2=1 3=1 4=1 5=1 6=1179648
BatchNorm		conv8/bn		1 1 conv8 conv8/bn_out 0=512
Scale			conv8/scale		1 1 conv8/bn_out conv8/scale_out 0=512 1=1
Relu			conv8/relu		1 1 conv8/scale_out conv8/relu_out 0=0.1

Convolution		conv9			1 1 conv8/relu_out conv9 0=18 1=1 2=1 3=1 4=0 5=1 6=9216
BatchNorm		conv9/bn		1 1 conv9 conv9/bn_out 0=18
Scale			conv9/scale		1 1 conv9/bn_out conv9/scale_out 0=18 1=1
Relu			conv9/relu		1 1 conv9/scale_out conv9/relu_out 0=0

Eltwise			eltw0			1 1 conv7/relu_out eltw0 0=0 1=1

Convolution		conv10			1 1 eltw0 conv10 0=128 1=1 2=1 3=1 4=0 5=1 6=32768
BatchNorm		conv10/bn		1 1 conv10 conv10/bn_out 0=128
Scale			conv10/scale	1 1 conv10/bn_out conv10/scale_out 0=128 1=1
Relu			conv10/relu		1 1 conv10/scale_out conv10/relu_out 0=0.1

Deconvolution	deconv0			1 1 conv10/relu_out 0=128 1=2= 3= 4= 5=  6= 7= 	